\begin{abstract}
    This article introduces a method for generating text, images, and videos by integrating image prompts and Large Language Models (LLMs) to address the challenges of appearance generation and spatiotemporal consistency. We propose a method of using LLM to refine prompts, decomposing abstract prompts into concrete prompts to enhance dynamic understanding. In addition, we have improved the global dynamics and content movement in generated videos by combining image guidance with position encoding and coarse to fine fusion strategies. A novel text image video dataset based on the WebVid-10M dataset is constructed for training and evaluation, and its performance is evaluated using the CLIP scoring metric. Our contributions include refining video summarization prompts, image-guided video generation, and dataset construction, significantly improving the quality of video generation. This work is of great significance for content creation, gaming, and film production, providing a promising direction for future research on text to video generation. Our work is already underway \hyperref[https://github.com/cxlhyx/DL-CLASS-PROJECT]{\textbf{https://github.com/cxlhyx/DL-CLASS-PROJECT}} upload.
\end{abstract}