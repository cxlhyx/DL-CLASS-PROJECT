\section{Related Works}
\label{sec:Related Works}
In this section, we will elaborate on the following aspects, diffusion models, text-to-video models, Text-to-Video Models with Image prompt.

\subsection{Diffusion Models}
Diffusion models are a class of likelihood-based generative models that have shown remarkable progress in image generating tasks \cite{rombach2022high,ruiz2023dreambooth}. The several models mentioned in the introduction are all based on diffusion models as baselines, additionally, diffusion models have a thriving research and application ecosystem, including multiple works \cite{mou2024t2i} as well as emerging open-source communities and libraries \cite{wang2023zero} with frameworks and plugins.\\
\indent The core of the diffusion model lies in two key steps: the forward diffusion process and the reverse generation process. The forward process gradually maps the data to a Gaussian distribution by adding noise, while the reverse process gradually denoises to restore the data through a parameterized network (such as U-Net).\\
\indent In the development of diffusion models, Denoising Diffusion Probabilistic Models (DDPM) is one of the earliest widely studied frameworks. DDPM trains the denoising process at each step by optimizing an objective function based on a variational lower bound. This method generates high-quality samples, but the computational cost is high, especially in the inference stage, which requires a large number of time steps (such as hundreds of steps) to complete denoising. In order to solve this problem, Denoising Diffusion Implicit Models (DDIM) proposes an improved method to reduce the number of generation steps, and significantly improves the generation speed by switching to a deterministic reverse process.\\
\indent Another important development in diffusion models is the combination of large-scale data and computing power for pre-training. In recent years, models such as Imagen and Stable Diffusion have demonstrated the power of diffusion models in text-to-image generation tasks. These models enable the generated images to be accurately matched to complex textual descriptions by combining a diffusion process with multi-modal embeddings of large language models such as Transformer. At the same time, the open source of these models promotes the practical application of diffusion models in art creation, game design and other fields.\\
\indent The applications of diffusion models are not limited to image generation. In video generation, diffusion models are used to generate consecutive video frames by extending the spatial dimension to the temporal dimension. Although diffusion models are powerful, they also face some challenges, such as spatiotemporal consistency issues. Therefore, how to further optimize the generation process, improve training efficiency and expand multi-modal capabilities is still a hot topic in current research.

\subsection{Text-to-Video Models}
Early explorations of text to video models \cite{hong2022cogvideo,villegas2022phenaki} were based on the idea of VKVAE, while recently, the emergence of diffusion models \cite{ho2020denoising,rombach2022high} has driven further research in this field \cite{an2023latent}. The diffusion based text to video model demonstrates stronger capabilities, such as Video Diffusion Models (VDM). These models improve temporal consistency while ensuring video frame quality by extending the diffusion process on the timeline. Imagen Video \cite{an2023latent} is based on Google's Imagen framework and uses a large-scale text to image generation model as a foundation to gradually generate high-quality videos by increasing the temporal dimension. Similarly, Meta's Make-a-Video \cite{singer2022make} uses a small amount of video data to extend the existing image generation model for temporal modeling, demonstrating the ability to generate high fidelity videos in different scenarios. These models typically rely on large-scale language visual pre training data, as well as efficient sampling and multi-scale generation strategies.\\
\indent The open-source development of Stable Diffusion has driven community research on Wensheng video models, with some projects combining Stable Diffusion with temporal modeling components such as Transformer or LSTM to generate high-resolution and frame consistent videos. In addition, research extending to the generation of long videos has gradually received attention, such as generating longer videos with better continuity through segmented generation and concatenation.\\
\indent In terms of technology, spcae-time consistency is a key direction. Some works enhance consistency by modeling the latent variable space over time (such as using CLIP or other visual language models to supervise each frame generation). In addition, domain specific fine-tuning based on real video data has been proven to effectively improve the diversity and reliability of model generation.

\subsection{Text-to-Video Models with Image prompt}
The image-based video model is of great significance in generative artificial intelligence. This type of model uses input text and static images as starting points to generate dynamic time series, providing creators with a natural transition from static to dynamic content. Compared to completely text-based generation, image prompts can provide more initial visual information, enhancing the model's understanding of scene layout, subject features, and details. This method is widely used in fields such as virtual reality, advertising creation, and video editing.\\
\indent Early research was typically based on GAN (Generative Adversarial Network) architectures, such as MoCoGAN \cite{tulyakov2018mocogan} and TGAN, which extended static images into continuous videos by decoupling image content and motion features. However, these methods face challenges in generating inter frame consistency and high-quality details, especially for the generation of long-term videos. With the development of VAE (Variational Autoencoder) and VKVAE (Vector Quantization Variational Autoencoder) technologies, latent variable based generative models provide new ideas for image to video generation. For example, by modeling the temporal sequence of latent variables in images, continuous video frames that conform to dynamic logic can be generated.\\
\indent In recent years, the rise of diffusion models has further promoted the development of image-based video models. The gradual denoising process of the diffusion model is very suitable for dealing with inter frame temporal consistency issues. For example, models such as Imagen Video \cite{ho2022imagen} and Make-A-Video \cite{singer2022make} embed static images into the generation process of diffusion models to generate high-quality dynamic videos while preserving input image details. In addition, these models are often combined with powerful multimodal pre training models (such as CLIP or large language models) to enable the generated videos to match auxiliary text descriptions, thereby providing higher semantic control.\\
\indent VideoBooth \cite{jiang2024videobooth} is also a type of image-based generative video model, which consists of two modules: a coarse embedding module through an image encoder and a fine embedding module through attention injection. The image encoder provides rough embedding of image prompts to refine the attention injection module. These two modules work together to train in a coarse to fine manner. In this article, we will use VideoBooth as a baseline for further research.