\section{Introduction}
\label{sec:Introduction}

Text-to-video models has attracted widespread attention as a type of deep learning model\cite{blattmann2023align,esser2023structure}, it generates videos by taking text descriptions as inputs. A video can provide more informative, attractive, and immersive visual content, which can benefit novel content creation, gaming, and movie production \cite{he2022latent}. Therefore, it is very necessary to study how to better use text to generate videos.\\
\indent Most existing studies start with simple text prompts only, such as Free-Bloom \cite{huang2024free}, Text2Video-Zero \cite{li2023amt}, both of them are zero-shot text-to-video Generators. In this way, there are some problems, we point out two obvious issues. One is models cannot generate the desired appearance, the other is space-time consistency issues.\\
\indent In order to solve the first problem, an intuitive method is to use images as references, called image prompts. Image prompts complement text prompts and enrich the details that are difficult to describe with text prompts. There are two main types of attempts at image prompts. One is to use a few-sample reference images to fine-tune some parameters \cite{choi2023custom}, and the other is to embed images into the video model and the inference is tuning-free \cite{chen2023disenbooth}. Jiang explore the task of video generation using image prompts without finetuning at inference time, and proposed VideoBooth \cite{jiang2024videobooth}, which generates consistent videos containing the desired subjects. However, during our experiments, we found that the videos generated by VideoBooth lacked global dynamics and the content was difficult to move.\\
\indent Free-Bloom generate high-quality, space-time consistent, and semantically aligned videos by harnessing large language models (LLMs) as the director, while pretrained image diffusion models as the frame animator. Referring to Free Bloom and VideoBooth, we propose video summary prompts and image prompts to video generation with adding DSL to guide video generation and using dynamic weights for conditional fusion.\\
\indent We conducted our method on the WebVid-10M dataset, which is a classic video dataset consisting of 10M video clips with a duration mostly within 30s. Each video is accompanied by a corresponding textual description and has a dataset format similar to OpenVid-10M. Overall, it contains information such as the video file name, text description, frame rate, duration, etc.\\
\indent In summary, our contributions can be summarized as:\\
\indent 1) Video Summary Prompt Refinement: We enhance video generation by leveraging LLMs to decompose abstract prompts into multiple concrete ones, which are encoded via CLIP to guide frame-by-frame generation, improving dynamics and abstract understanding.\\
\indent 2) Image-Guided Video Generation: Introducing position encoding, incorporating image guidance alongside text prompts and adopting a coarse-to-fine fusion strategy inspired by VideoBooth.\\
\indent 3) Data augmentation: Constructed a dataset for image guided video generation.\\
\indent This article will start from these aspects: Section 1 is the introduction, Section 2 is the related work, Section 3 we provide a detailed description of our method, Section 4 we conducted our method on the WebVid-10M dataset, Section 5 is the discussion about our method, Section 6 we summarize our article.