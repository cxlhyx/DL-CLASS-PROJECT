\section{Experiment}
In this section, we will describe our improvement on dataset and model. In addition, we provide results and indicators about our model.

\subsection{Dataset}
In the text-to-video model generated by image guidance, text and image are two important prompts, which are also the inputs of the model. Nowadays, most video datasets are just text-to-video datasets, lacking the image of the video subject, so such datasets cannot be used as input to train textual video models generated by image guidance. To solve this problem, we construct a text-image-video dataset based on the classic video dataset WebVid-10M. The text-image-video dataset includes videos, their textual descriptions, and the subject's location coordinates in the video, which link to the corresponding image. We construct the dataset as follows: first, we download the video from the WebVid-10M dataset and extract the corresponding text, then we filter the text description of the video according to the keyword, filter out the video with a certain keyword in the corresponding text, and use the keyword as the label of the image prompt of the video, and then we extract the frame of the video, and finally use the object detection model Fasterrcnn\_ Resnet50 detects the video frame, the detection target is the label of the video image prompt, and finally returns the coordinates of the detection target box to obtain the text-image-video dataset.

\subsection{Results and Analysis}
In order to verify the performance of the model, we evaluated the model under a RTX4090 condition, and because the CLIP-Score is closer to the human perception standard, we chose the CLIP-Score as the comparison metric. CLIP-Score is an important indicator to measure the alignment of text and image, and the CLIP model is used to map the text and image into the same vector space, and then calculate the cosine similarity of the two. Therefore, the higher the CLIP-Score, the better the text-image alignment. For the CLIP-Score of the video, we extract all the frames of the generated video, calculate the CLIP-Score for each frame of the image, and finally take the average value to obtain the CLIP-Score of the model.

Firstly, we compare the performance of the model before and after the introduction of relative position coding, the CLIP-Score of the model before the introduction of relative position coding is 25.69, and the CLIP-Score of the model after the introduction of relative position coding is 25.7049.

Then we compared the same CosisID and ID-Animator models with the same image as the guide, mainly in terms of the alignment of the video image prompt, the experimental results are as followsw.

\begin{table}
    \centering
    \label{Comparison}
    \caption{Comparison}
    \begin{tabular}{|c|c|}
        \hline
            & CLIP-Score \\
        \hline
            ID-Animator & 24.97 \\
        \hline
            CosisID & 27.93 \\
        \hline
            VideoBooth (our) & 25.70 \\
        \hline
    \end{tabular}
\end{table}

As can be seen from the \hyperref[Comparison]{Table 2}, CosisID \cite{yuan2024identity} has the highest CLIP-Score score, because VideoBooth embeds image prompts into text prompts by coarse-to-fine methods, resulting in CLIP-Scores of videos, images and texts 2.8\% higher than ID-Animator \cite{he2024id}.